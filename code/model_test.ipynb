{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "data_frame=pd.read_csv(\"../data/test_data1_2type.csv\",\n",
    "                       names=['category', 'text'],\n",
    "                       index_col=False\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_categories=data_frame.category.unique()\n",
    "possible_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0, 5: 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标签映射\n",
    "label_dict={}\n",
    "for index, possible_category in enumerate(possible_categories):\n",
    "    label_dict[possible_category]=index\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict[5]=1\n",
    "label_dict[1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>还好吧，不喜欢里面的中国元素。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>名气很高，但真实情况是？失望！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>不喜欢阿三</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>我还以为英国的电影呢我是看一群帅哥的面才看完这部电影的。电影的故事不外乎成人世界压抑天性，正...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>讲真，美国真是好人多，中国真是坏人多，这种人在中国太多了，估计一大堆做假的各种领导学者在中国...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>这部电影对于我个人非常有纪念意义，它令我惊觉自己的改变。那种宁愿守着旧梦死去也不愿意走向迷茫...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>就是个小时长的日剧嘛。拉大提琴的几场尤其造作尤其傻，这种半调子剧本、台词、烂表演、配这种业余...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>很低俗，猥琐，，，周星驰就爱这个是吧，我几乎要放弃看他的作品了。只有黄圣依美是真美，但目的呢...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>就靠各类动物的体型差和一路不停的耍宝来制造萌点吧。食肉动物的本性本来就是野蛮：树獭一点也不好...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>昏昏欲睡。以为会对自己的刑法理论有一定的冲击，但失望了，讲的不就是罪刑法定原则吗？需要拍的那...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text  label\n",
       "0         1                                    还好吧，不喜欢里面的中国元素。      0\n",
       "1         1                                    名气很高，但真实情况是？失望！      0\n",
       "2         1                                              不喜欢阿三      0\n",
       "3         1  我还以为英国的电影呢我是看一群帅哥的面才看完这部电影的。电影的故事不外乎成人世界压抑天性，正...      0\n",
       "4         1  讲真，美国真是好人多，中国真是坏人多，这种人在中国太多了，估计一大堆做假的各种领导学者在中国...      0\n",
       "5         1  这部电影对于我个人非常有纪念意义，它令我惊觉自己的改变。那种宁愿守着旧梦死去也不愿意走向迷茫...      0\n",
       "6         1  就是个小时长的日剧嘛。拉大提琴的几场尤其造作尤其傻，这种半调子剧本、台词、烂表演、配这种业余...      0\n",
       "7         1  很低俗，猥琐，，，周星驰就爱这个是吧，我几乎要放弃看他的作品了。只有黄圣依美是真美，但目的呢...      0\n",
       "8         1  就靠各类动物的体型差和一路不停的耍宝来制造萌点吧。食肉动物的本性本来就是野蛮：树獭一点也不好...      0\n",
       "9         1  昏昏欲睡。以为会对自己的刑法理论有一定的冲击，但失望了，讲的不就是罪刑法定原则吗？需要拍的那...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame['label']=data_frame.category.replace(label_dict)\n",
    "data_frame.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>还好吧，不喜欢里面的中国元素。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>名气很高，但真实情况是？失望！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>不喜欢阿三</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>我还以为英国的电影呢我是看一群帅哥的面才看完这部电影的。电影的故事不外乎成人世界压抑天性，正...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>讲真，美国真是好人多，中国真是坏人多，这种人在中国太多了，估计一大堆做假的各种领导学者在中国...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>很悲伤的故事，爸爸的爱让人感动又心痛！为什么可爱的孩童受到伤害后还得不到守护！</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>太好看了！今年最喜欢的一部电影！还好憋着没去看网络版一直等着上映，大银幕看那美轮美奂的画面真...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>当Joker决心暗杀Thomas Wayne时，我们发现了反向的双子杀手的结构：缺失的父亲之...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>有生之年能够大荧幕四刷，太幸福了。人生哪有什么机缘巧合，全是辜负和错过。以前不喜欢刘嘉玲，但...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>你什么时候变得这么罗嗦，以前都不是这样。 傻女，听我说，现在立刻回家，洗个热水澡， 明早起来...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0                                       还好吧，不喜欢里面的中国元素。      0\n",
       "1                                       名气很高，但真实情况是？失望！      0\n",
       "2                                                 不喜欢阿三      0\n",
       "3     我还以为英国的电影呢我是看一群帅哥的面才看完这部电影的。电影的故事不外乎成人世界压抑天性，正...      0\n",
       "4     讲真，美国真是好人多，中国真是坏人多，这种人在中国太多了，估计一大堆做假的各种领导学者在中国...      0\n",
       "...                                                 ...    ...\n",
       "1595            很悲伤的故事，爸爸的爱让人感动又心痛！为什么可爱的孩童受到伤害后还得不到守护！      1\n",
       "1596  太好看了！今年最喜欢的一部电影！还好憋着没去看网络版一直等着上映，大银幕看那美轮美奂的画面真...      1\n",
       "1597  当Joker决心暗杀Thomas Wayne时，我们发现了反向的双子杀手的结构：缺失的父亲之...      1\n",
       "1598  有生之年能够大荧幕四刷，太幸福了。人生哪有什么机缘巧合，全是辜负和错过。以前不喜欢刘嘉玲，但...      1\n",
       "1599  你什么时候变得这么罗嗦，以前都不是这样。 傻女，听我说，现在立刻回家，洗个热水澡， 明早起来...      1\n",
       "\n",
       "[1600 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_col=['text','label']\n",
    "sub_df=data_frame[selected_col]\n",
    "sub_df=sub_df.dropna()\n",
    "# sub_df=sub_df.sample(frac=0.5)\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载Tokenizer和Encode the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\DHW\\.conda\\envs\\d2l\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset_test): 1600\n"
     ]
    }
   ],
   "source": [
    "# 下载的预训练文件路径\n",
    "BERT_PATH = '../Models/bert-base-chinese'\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n",
    "\n",
    "\n",
    "# 将strings转化为tokens\n",
    "encoded_data_test = tokenizer.batch_encode_plus(\n",
    "    sub_df.text.values,\n",
    "    add_special_tokens=True,    # This is just the BERT way of knowing that when the sentence ends and when the a new one begins.\n",
    "    return_attention_mask=True, # 为将不同长度的句子拥有相同的维度，将max_length设置为很大的数字256，\n",
    "    pad_to_max_length=True,      # attention_mask表示实际值在哪里，同时哪里为空值\n",
    "    max_length=256,\n",
    "    return_tensors='pt' # pt表示PyTorch\n",
    "    )\n",
    " \n",
    "\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(sub_df.label.values)\n",
    "\n",
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "\n",
    "print(\"len(dataset_test):\", len(dataset_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0, 5: 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../Models/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    BERT_PATH,\n",
    "    num_labels = len(label_dict),\n",
    "    output_attentions=False,    # 是否输出注意力分散\n",
    "    output_hidden_states=False  # 是否输出模型的隐藏状态\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# model放到cuda中\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 48\n",
    "dataloader_test = DataLoader(dataset_test,\n",
    "                              sampler=RandomSampler(dataset_test),\n",
    "                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the performace Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import f1_score,accuracy_score, recall_score, precision_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    labels_dict_inverse = {v: k for k,v in label_dict.items()}\n",
    "\n",
    "    preds_flat = np.argmax(preds,axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {labels_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])/len(y_true)}\\n')\n",
    "\n",
    "def accuracy_score_func(preds, labels): # 准确率\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    labels_flat=labels.flatten()\n",
    "    labels_num=preds_flat.size\n",
    "    accurate_num=(preds_flat==labels_flat).sum()\n",
    "    print(\"total label num:\", labels_num)\n",
    "    print(\"error num:\", labels_num-accurate_num)\n",
    "    print(\"准确率（accuracy）:\",accuracy_score(labels_flat, preds_flat))\n",
    "\n",
    "def recall_score_func(preds, labels):   # 召回率\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    print(\"召回率（recall）:\",recall_score(labels_flat, preds_flat))\n",
    "\n",
    "def precision_score_func(preds, labels):    # 精确率\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    print(\"精确率（precision）:\",precision_score(labels_flat, preds_flat))\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in dataloader_val:\n",
    "\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total/len(dataloader_val)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载训练好的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path='./bert_checkpoint'\n",
    "\n",
    "model.load_state_dict(torch.load(f'{save_path}/bert-base-chinese_2type_39200_epoch_6.model'))\n",
    "# model.load_state_dict(torch.load(f'{save_path}/bert-base-chinese_2type_20000num_epoch_10.model'))\n",
    "# model.load_state_dict(torch.load(f'{save_path}/bert-base-chinese_2type_39200_epoch_6.model',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入单个文本进行预测分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction for label: 0\n"
     ]
    }
   ],
   "source": [
    "def predict_single_example(model, tokenizer, text):\n",
    "    # 使用 tokenizer 对文本进行编码\n",
    "    # inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    encoded_data=tokenizer.encode_plus(\n",
    "        text=text,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = encoded_data['input_ids']\n",
    "    attention_masks=encoded_data['attention_mask']\n",
    "\n",
    "    inputs = {\n",
    "            'input_ids': input_ids.to(device),\n",
    "            'attention_mask': attention_masks.to(device),\n",
    "        }\n",
    "    \n",
    "    # 将编码后的数据传递给模型进行预测\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # 获取模型输出中的分类概率\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    # print(probabilities.flatten().tolist())\n",
    "    # 获取预测的类别\n",
    "    predicted_class = torch.argmax(probabilities).item()\n",
    "\n",
    "    print(\"predicted_class:\",predicted_class)\n",
    "\n",
    "def predict_single_example_2(model, tokenizer, text):\n",
    "    # 使用 tokenizer 对文本进行编码\n",
    "    # inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    encoded_data=tokenizer.encode_plus(\n",
    "        text=text,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = encoded_data['input_ids']\n",
    "    attention_masks=encoded_data['attention_mask']\n",
    "    inputs = {\n",
    "            'input_ids': input_ids.to(device),\n",
    "            'attention_mask': attention_masks.to(device),\n",
    "        }\n",
    "    \n",
    "    # 将编码后的数据传递给模型进行预测\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # loss = outputs[0]\n",
    "    # print(outputs)\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    preds_flat = np.argmax(logits).flatten().item()\n",
    "    print(\"prediction for label:\", preds_flat)\n",
    "\n",
    "        \n",
    "# 示例使用\n",
    "# text_to_predict = \"这个剧算是我2023年最大的惊喜，本以为翻拍剧会不及韩国人那么会营造氛围感，结果这个剧令我相当意外，不愧是文艺片演员来执导。作为春夜剧粉，客观来说，我觉得张晚意孙怡以及一众配角要比原版的好看。BGM和男主的声音，都太好听了。一见钟情时无声无息，但已经暗潮汹涌。第一场雪是你，心动是你，纠结是你，无法自控忍不住想靠近的也是你。看得我很想谈恋爱。。。\"\n",
    "text_to_predict = \"对爆米花商业电影真的一点感觉都没有啦\"\n",
    "# predict_single_example(model, tokenizer, text_to_predict)\n",
    "predict_single_example_2(model, tokenizer, text_to_predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试集上评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 1\n",
      "Accuracy: 0.86125\n",
      "\n",
      "Class: 5\n",
      "Accuracy: 0.93375\n",
      "\n",
      "total label num: 1600\n",
      "error num: 164\n",
      "准确率（accuracy）: 0.8975\n",
      "召回率（recall）: 0.93375\n",
      "精确率（precision）: 0.8706293706293706\n",
      "f1 score: 0.8973651313679383\n"
     ]
    }
   ],
   "source": [
    "_, predictions, true_vals = evaluate(dataloader_test)\n",
    "accuracy_per_class(predictions, true_vals )\n",
    "accuracy_score_func(predictions, true_vals)\n",
    "recall_score_func(predictions, true_vals)\n",
    "precision_score_func(predictions, true_vals)\n",
    "val_f1 = f1_score_func(predictions, true_vals)\n",
    "print(\"f1 score:\", val_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
