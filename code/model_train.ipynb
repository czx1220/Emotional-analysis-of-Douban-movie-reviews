{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "data_frame=pd.read_csv('../data/test_data1_3type.csv', \n",
    "                       names=['category', 'text'],\n",
    "                       index_col=False\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 5], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_categories=data_frame.category.unique()\n",
    "possible_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0, 3: 1, 5: 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标签映射\n",
    "label_dict={}\n",
    "for index, possible_category in enumerate(possible_categories):\n",
    "    label_dict[possible_category]=index\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>还好吧，不喜欢里面的中国元素。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>名气很高，但真实情况是？失望！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>不喜欢阿三</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>我还以为英国的电影呢我是看一群帅哥的面才看完这部电影的。电影的故事不外乎成人世界压抑天性，正...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>讲真，美国真是好人多，中国真是坏人多，这种人在中国太多了，估计一大堆做假的各种领导学者在中国...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>这部电影对于我个人非常有纪念意义，它令我惊觉自己的改变。那种宁愿守着旧梦死去也不愿意走向迷茫...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>就是个小时长的日剧嘛。拉大提琴的几场尤其造作尤其傻，这种半调子剧本、台词、烂表演、配这种业余...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>很低俗，猥琐，，，周星驰就爱这个是吧，我几乎要放弃看他的作品了。只有黄圣依美是真美，但目的呢...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>就靠各类动物的体型差和一路不停的耍宝来制造萌点吧。食肉动物的本性本来就是野蛮：树獭一点也不好...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>昏昏欲睡。以为会对自己的刑法理论有一定的冲击，但失望了，讲的不就是罪刑法定原则吗？需要拍的那...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text  label\n",
       "0         1                                    还好吧，不喜欢里面的中国元素。      0\n",
       "1         1                                    名气很高，但真实情况是？失望！      0\n",
       "2         1                                              不喜欢阿三      0\n",
       "3         1  我还以为英国的电影呢我是看一群帅哥的面才看完这部电影的。电影的故事不外乎成人世界压抑天性，正...      0\n",
       "4         1  讲真，美国真是好人多，中国真是坏人多，这种人在中国太多了，估计一大堆做假的各种领导学者在中国...      0\n",
       "5         1  这部电影对于我个人非常有纪念意义，它令我惊觉自己的改变。那种宁愿守着旧梦死去也不愿意走向迷茫...      0\n",
       "6         1  就是个小时长的日剧嘛。拉大提琴的几场尤其造作尤其傻，这种半调子剧本、台词、烂表演、配这种业余...      0\n",
       "7         1  很低俗，猥琐，，，周星驰就爱这个是吧，我几乎要放弃看他的作品了。只有黄圣依美是真美，但目的呢...      0\n",
       "8         1  就靠各类动物的体型差和一路不停的耍宝来制造萌点吧。食肉动物的本性本来就是野蛮：树獭一点也不好...      0\n",
       "9         1  昏昏欲睡。以为会对自己的刑法理论有一定的冲击，但失望了，讲的不就是罪刑法定原则吗？需要拍的那...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame['label']=data_frame.category.replace(label_dict)\n",
    "data_frame.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>不知道是剧情不好 还是我不喜欢这风格 勉强看完</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>序幕一上来就对农场主奴隶制的逝去时光表达了一波怀旧，真是让人浑身难受，还A civiliza...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>为什么常年位居圣诞电影推荐榜单上啊，剧情又臭又长的</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>杀手不冷，萝莉坚忍，他们的那种爱与不能，细腻、动人、深刻。这种最内敛最纯粹最真挚的情感，融于...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>没有人是逃避着走向人生的终点的。父亲就如同那条大鱼，向往最热切的自由和对生活的企盼，终其一生...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>这一部一点都不entertaining，父与子在音乐和画面重叠的转场下在时间的波涛中相望。时...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>影片足以炫耀的东西不够多，但是拍得不错，而且有趣的元素不少，影片更注重对白而不是紧凑的动作戏。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>SIFF 重看降一星 吴宇森大概只会拍这种直男魅力片 不过看到最后还是迷之感动</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>评分有点虚高，这些jc智商不在线，纯属是搞笑的，尤其一个男的还要和那个女的，开车还掉了还挺讽...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>我在你心里是个句号还是一个惊叹号？</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "324                             不知道是剧情不好 还是我不喜欢这风格 勉强看完      0\n",
       "466   序幕一上来就对农场主奴隶制的逝去时光表达了一波怀旧，真是让人浑身难受，还A civiliza...      0\n",
       "677                           为什么常年位居圣诞电影推荐榜单上啊，剧情又臭又长的      0\n",
       "1881  杀手不冷，萝莉坚忍，他们的那种爱与不能，细腻、动人、深刻。这种最内敛最纯粹最真挚的情感，融于...      2\n",
       "881   没有人是逃避着走向人生的终点的。父亲就如同那条大鱼，向往最热切的自由和对生活的企盼，终其一生...      1\n",
       "...                                                 ...    ...\n",
       "1406  这一部一点都不entertaining，父与子在音乐和画面重叠的转场下在时间的波涛中相望。时...      1\n",
       "1268    影片足以炫耀的东西不够多，但是拍得不错，而且有趣的元素不少，影片更注重对白而不是紧凑的动作戏。      1\n",
       "1472            SIFF 重看降一星 吴宇森大概只会拍这种直男魅力片 不过看到最后还是迷之感动      1\n",
       "942   评分有点虚高，这些jc智商不在线，纯属是搞笑的，尤其一个男的还要和那个女的，开车还掉了还挺讽...      1\n",
       "1847                                  我在你心里是个句号还是一个惊叹号？      2\n",
       "\n",
       "[720 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_col=['text','label']\n",
    "sub_df=data_frame[selected_col]\n",
    "sub_df=sub_df.sample(frac=0.3)\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集、验证集、测试集 \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(sub_df.index.values, \n",
    "                                                    sub_df.label.values,\n",
    "                                                    test_size = 0.15, \n",
    "                                                    random_state=17, \n",
    "                                                    stratify = sub_df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 text\n",
       "label data_type      \n",
       "0     train       195\n",
       "      val          34\n",
       "1     train       221\n",
       "      val          39\n",
       "2     train       196\n",
       "      val          35"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df['data_type']=['not_set']*sub_df.shape[0]\n",
    "sub_df.loc[X_train, 'data_type']='train'\n",
    "sub_df.loc[X_val, 'data_type']='val'\n",
    "sub_df.groupby(['label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAETCAYAAADXmaY8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAaPUlEQVR4nO3de7hcVX3G8e9LbgYI99NIFAhIRCsQL6mEGGxAUm5yVRFRsQUarXih1kcRAS2FChQREFGjKSgqChUQuTaIYBCwBFQoFWuqQUhBo0BiKMolv/6x1iGTyZ5zkpzZM3POej/Pc54zs/eevX5nZ/LOmrX27FFEYGZmI9sG3S7AzMzq57A3MyuAw97MrAAOezOzAjjszcwK4LA3a0HSxt2uwaxdHPZWG0l7D7L+PElHNC27QtIb1mLfX5T0d2ux3b9L2qvh/raSviNp/CCP+yvghsH2n7d9Wf69Q2NbA2w/StKtkrZbm/3nx2woadRabruBpFlru28rw+huF2Ajk6RxwGmStoyIb1Wsnwy8G9hD0vtIHY9/Bd4IbCMJIIArIuKMiiaezj+DeX47SWOAbwL3RcRTTfW8ETilYZ+jgJ0l3ZbvjwPmR8SJTY/bH/gXSTvneudK2jUi/m+AmvYFNoyIB1ttIGk+sAnwTF40BXhK0sP5/hjgkYg4pOLhGwLflHRQRPxHi/3fQ/r/P9Ax3CYiJg6w3oYR+UNV1i6SfkgKqD8NsNnWwI7AfOAi4Cbg+8CfA5fn+98FrgdeHRFPNuz/CeB+UqhuDzwJ/Davfk1ErNFbl3QNcEZE3CbpImBz4E0R8VzTdkcC0yLiQ5K2AMZExG8kbZDrWFix7w2AO4HTI+I7edl5wPiImNOw3e7ApcAK4FngpcAjwB8ad0cK3+Mi4paKtm4APhERP2pRx+iIeLph2ZHAbyLiew3LRgMrI2KlpB8Bb42IxZJmACcD+0cOhLzt4oh4cXN7Njy5Z2/tFMChEbGoamUOkF8BE4GFETFP0kHAFcC2wIPAhcBY4ErgRcB/N+ziaeANEfFHSecCP4mIi/O+lw9UmKTzgZcBezUHff8mQOR3JG/JbZ+S110jaWtSoD7T8JgPA8v6gz77OHCnpE8DH47kDmByruNtwNERMVvSl4EPtHoXIOko4EP57hTgYkn9L6RfjYhz8u2/AC6RtJLUq98cWJL30bjLUcBhwH3kdwySNgW+BDwF3JWHpD4OfI70wmQjhMPe2ukw4CWSvk0K/n6bksLuCOAVEbEcOD73eL9A6lH/L/A+AEl7AttHRGPQA9wD3JQDbHtgf0nH5nXP97xzT1dNof4z4NTG4Zs8rPNcRKwkhf2zwC3AlsBYSa/Pm24O3ArcBpyYH/sG4HjgdY0FRsSKPG4/H/iBpGMj4uf5MZOBfwJm5833bDpOzTYDLgPOalp+FOndQX+bP+q/n+dJ5kTE4QPst98o0ruox4AFpHcpxwKfxfN5I47D3tomIn5LGlaZ2r9M0iHAJ4G3N47dS9qENIxzJnCGpF1Z1ZOcAGwl6V7g7Ij4at7/vg2PP5eGnn2TPYCLJD0NvBjYhTS0dL6k/2nYbhzpBerHwAuAP0bE7pLmANNJcwgAl0VEf/AjaQIwlzT0cUee7BXQ/+5iC+DvgWnAc/kxWwPXAtsBV+YXrBeT3gUE6Z3NURFxTUN9K4E5wP5Nf9+fkV4EqmwE7CPpgabliyLijU3LngPeD/yO9EJ2PHBAHuZx2I8wDntrG0ljm8aNDyP1ZGdFxNK8bBSpx/ptUjAti4ijJG1GGk9ens8kOTYi3tGwrznAJ0jDQLBmz34K8MGI+GZE3ArskB/XOGZ/H3B4RPysovyNgN9KGgv8hBTYM/O61SaII+IPkl6e/9Z5ks4ClkTEebnNG4CfRcQX8/2ppKGqc4GPRsQr8/JFwG55WOpiquc6fgzc2LRs94rt+m0LfDoiTu1fIGkm8I8V225DeiEG+AbwQ+DdkuaS32XZyOGwt7aQtDmpl/vHhsWbkIZE5jeMHb+AFCw3kXqT/U4hjTe/p0UTzwCXR8Txub1zWX3M/mIGnhiG9C7iQkl7V4zbvwi4gzRkcmBe9lD+vZukpRFxaf/GjS9qwF8CH2za18MN95cBJ0bEtyR9dID6qoZ0fg80z4HsMMA+9gY+3bTshaQJ4WYPkYaE/hd4J/DPpHcscxj8WNow47C3toiIx0kToM/LpzMe2+L0QCSd1nD3VOCruWdduTnwttxLhdQrPTiftglpTuCaqgc21PJL0hkwX5F0TEQ0BtquwCURcaWkM4GrgcOBfUinOF66xk7Tfg8GRkXEnQ2LtyZPkAJExGJgcb67xvBInhTekuoJ0X2BVzYt2xL4esV+XksaJlrQtGoiq7/4QDqeRMSS/AL0JtK7rVvzu6vR/dvYyOCwt24Sq0LnCeAggDyk03z+9/3AMf1j2vnsmnsaevZvAX5R0cZYUo9+O9J5/W8jhdrdkj5OmqCcQDr18/5cyyOSPkGa9H2MFsMmOegvBA7J9zcBdiaN/bfqGY9ruD2adL78vaTx/nvzfhqD9oLmzxlI+mtgx7wdEfGspB1JLwBHNpw+uTFpOOpQ0ucLGo3pvxERZ+bTUt8O7NR/HHA+jCj+x7RaSPoI6ayTlh8cIgXfaj35HOJvIgXz8yrOLx/dtP7yiho2IL3buBHYJyJW5OX7Ae8lnUlzNbBf/j0zvwN4Ra7tM6TJ3bsk/Rb4Del0yiV5GGlv4KCIuDs3+R7SsMhqH7xqMqnh9pj8d7y06ZTOg0nzE/0fBntzi30dCJwt6TrgKuBvI+KuhvWnkYL+Ztac0B0NXJcnsVcj6ZSGbWyE8IeqrBa517slcFVEPLYOjxvTFHxDrWNSPq1z0HaBrUhnEt3dP6HcsH4HYEpE3JjvTyD14NtW61A0T46vxfZbAk+0+MyBjUAOezOzAvhcWjOzAjjszcwK0JMTMFtttVVMnjy522WYmQ0rd9999+8ioq9qXU+G/eTJk1m4cI2LDJqZ2QAktTz7zcM4ZmYFcNibmRXAYW9mVgCHvZlZARz2ZmYFcNibmRXAYW9mVgCHvZlZAYoM+8knXNvtEszMOqrIsDczK43D3sysAA57M7MCOOzNzArgsDczK4DD3sysAA57M7MCOOzNzArgsDczK4DD3sysAA57M7MCOOzNzArgsDczK4DD3sysAA57M7MCOOzNzArgsDczK4DDvoG/wcrMRiqHvZlZARz2ZmYFcNibmRXAYW9mVgCHvZlZARz2ZmYFcNibmRXAYW9mVgCHvZlZARz2ZmYFqCXsJW0u6TpJCyR9IS+bJ+l2SSfV0WYVX/7AzCypq2f/TuBrEbEHMEHSR4BRETEDmCRpSk3tmplZhbrC/vfATpI2A7YBJgOX5XU3AzObHyBpjqSFkhYuXbq0prLMzMpUV9jfBkwBPgA8AIwDluR1y4GJzQ+IiLkRMS0ipvX19dVUlplZmeoK+38G3hMRp5LC/khgfF63cY3tmplZhbpCd0NgF0mjgN2AM1g1dDMVWFxTu2ZmVmF0Tfv9FHARsB1wB/AZYIGkScB+wPSa2jUzswq1hH1E/AfwisZlkmYBs4GzImJZHe2amVm1unr2a4iIx1l1Ro6ZmXVQUROl/pCVmZWqqLA3MyuVw97MrAAOezOzAjjszcwK4LA3MyuAw97MrAAOezOzAjjszcwK4LA3MyuAw97MrAAOezOzAjjszcwK4LA3MyuAw97MrAAOezOzAjjszcwK4LA3MyuAw97MrAAOezOzAjjszcwK4LA3MyuAw97MrAAOezOzAjjszcwK4LA3MyuAw97MrAAOezOzAjjszcwK4LA3MyuAw97MrAAO+xYmn3Atk0+4tttlmJm1hcPezKwAxYe9e/BmVoLiw97MrAS1hr2kCyUdmG/Pk3S7pJPqbNPMzNZUW9hL2gN4YUR8V9JhwKiImAFMkjSlrnbNzGxNtYS9pDHAl4DFkg4GZgGX5dU3AzMrHjNH0kJJC5cuXVpHWWZmxaqrZ38U8F/AWcBrgeOAJXndcmBi8wMiYm5ETIuIaX19fTWVZWZWptE17fdVwNyIeFTS14AZwPi8bmM8MWxm1lF1he4iYId8exowmVVDN1OBxTW1a2ZmFerq2c8D/lXSEcAY0pj91ZImAfsB02tq18zMKtQS9hHxB+AtjcskzQJmA2dFxLI62jUzs2p19ezXEBGPs+qMHDMz6yBPlJqZFcBhb2ZWAIe9mVkBHPZmZgVw2JuZFcBhb2ZWAIe9mVkBHPZD5G+5MrPhwGFvZlYAh72ZWQEGDXsl01qs2779JZmZWbutbc/+O5LOkfQ+SX8BIOk1wDfqK83MzNplwAuhSVJEhKRFwBeB7YD9JH0BeJqmK1uamVlvGqxnf72kfwM2BXYkXYd+GvBd4GGgmO8P9Fk3ZjacDXaJ4zeTvmXqOOBU4BHg4Ih4TtJk0heUvCEiotYqzcxsSAYL+/eSevS/B+4FzgV+JenrwM7ARx30Zma9b7BhnE3y7xnAONKLw0+Bn5B6/PfXVZiZmbXPYGE/H/gV8CrgAeBQ0heG7wucBZxYa3VmZtYWg4X9XqSzbi4AdgIuARYBd0bEJcBLJPmDWWZmPa7lmL0kAQuAO4AtgW3y708CD+aQvy8iVnagTjMzG4LBJmiPj4ibJF0APN6w/BFgM2BxTXV1nE+tNLORrOUQTD7LZmtJrwP+D/gUqWf/J1LYL4iIL3WkSjMzG5LBevYCXg1sD4wHLgbGArsAx0j6z4h4uNYKzcxsyAYas98A+E1EfDaP358MBOkFYBHwt8A8YJ9OFGpmZutvoGGclcC7JH0uIs4HppDOxtkCuCoiFpEma4clj9GbWUkGO23yQOB1ko7K276E9IGq70naIyLuqLk+MzNrg8HC/inSNXGeBPpPsbwf+CBwmqSJNdZmZmZtMljY/xp4N/A3wBjS5OzRpOvknAl8rNbqzMysLQY8GycibiNPwEo6MiK+Iely0ovEDcBj9ZdoZmZDNdipl8+LiG/k340frrqz7RWZmVnb+bo2ZmYFcNivo8knXOvTNs1s2HHYm5kVoLiwr6tX7h6/mfWy4sLezKxEtYW9pImSfpxvz5N0u6ST6mrPzMxaq7NnfzYwXtJhwKiImAFMkjSlxjbNzKxCLWEvaS/SJRYeBWYBl+VVNwMzWzxmjqSFkhYuXbq0jrI6yuP3ZtZL2h72ksYCpwAn5EUbAUvy7eVA5fV0ImJuREyLiGl9fX3tLsvMrGh19OxPAD4XEU/k+ytIX3wCsHFNbZqZ2QDW+nIJ62BvYC9JxwGvBLYFHiJdWmEq8PMa2lwvHmoxs1K0Pewj4vX9tyXdAhwELJA0CdgPmN7uNs3MbGC1DqlExKyIWE6apL0T2DMiltXZZq/xh63MrBfUMYyzhnylzMsG3dDMzGrhyVIzswI47M3MCtCRYZxu85i5mZXOPXszswKMyLBvd0/e7wzMbLgbkWFvZmarc9ibmRXAYW9mVgCHvZlZARz2ZmYFcNibmRXAYd/Ep1ma2UjksDczK4DD3nqaLxFt1h4OezOzAoz4sHevcHjyv5tZe434sDczM4e9mVkRHPYVOjmE4OEKM+sEh72ZWQEc9mZmBXDYm5kVwGFvZlYAh72ZWQEc9j3OZ+uszsfDbP047M3MCuCwH4R7kmY2EjjszcwK4LA3MyuAw34d1Dmk4+EiM6uTw97MrAAO+/W0vj1x9+DNrBsc9mZmBXDYDyP+PlYzW18OezOzAtQS9pI2lXS9pPmSrpQ0VtI8SbdLOqmONm3tdfMdgt+dmHVHXT37twPnRMRs4FHgCGBURMwAJkmaUlO7ZmZWoZawj4gLI2J+vtsHvAO4LN+/GZjZ/BhJcyQtlLRw6dKlQ66hE73HqjYalw3ljJ11eexQe8vd7Gm7l2/WGbWO2UvaHdgceAhYkhcvByY2bxsRcyNiWkRM6+vrq7MsM7Pi1Bb2krYAPgscDawAxudVG9fZrpmZramuCdqxpGGbj0XEg8DdrBq6mQosrqPdbum1oYhW9QxWpydPzUauunrYxwCvAT4u6RZAwDslnQMcDjhRzMw6aHQdO42IzwOfb1wm6WpgNnBWRCyro12rT3+Pf/EZBwzL/ZuVrpawrxIRj7PqjBwzM+sgT5T2kHU93XJtthkpY/Aj5e8w6xaHvZlZARz2NXAv1Mx6jcPezKwAHZugtfZcSsHMbH24Z29mVgCHvZlZARz2ZmYFcNibmRXAYW8tresHt5onoIdyPf/1rcfMqjnszcwK4LDvgnb1ULv5PbKdeIyZtY/D3sysAA77DqmrZzsSPqjlMXqz+jnszcwK4LAfJta2l7u+l0lufly7z6RZ233W0a6ZOezNzIrgsDczK4DDvges6xBELw5Z9GJNZraKw97MrAAO+x5T1UPupV6zP1BlNjw57M3MCuCwL9xgve71OZXSPXmz3uOwNzMrgL+DtocNpYfc6gNTvTLm7t6/WWe5Z29mVgCHva2hnb3u4diDH+olH8yqDOULfdrBYW9mVgCHvZlZARz2tl56YTijHW+Lu/F39MKxs/I47M3MCuCwt2FpXS8r0f8uoM7r9LfrOwHWp4bhsP86tGvSs9uTp53gsDczK4DD3oadtbnEw2AfJBvsUg+N7wSqeuzrc1nqVnW02n+7eprruq/Btu9UD7j537BXet4DPb+q/u16pW6HvZlZATp6uQRJ84CXA9dFxGmdbNtGroF6WYvPOGDA7avur007A+2/cduB2qq63Wp/rbZrrqHq/mB/R6t2B9pXYx1Vt6v2vzbHa6B6BqqtuabBtm21/WB/y2DHtmr/A9U/2L9fO3WsZy/pMGBURMwAJkma0qm2zcxKp4joTEPS+cANEXGdpDcDEyLioob1c4A5+e5OwM/Xs6mtgN8NqdjOca31cK31cK31aGet20VEX9WKTg7jbAQsybeXAzs2royIucDcoTYiaWFETBvqfjrBtdbDtdbDtdajU7V2coJ2BTA+3964w22bmRWtk4F7NzAz354KLO5g22ZmRevkMM5VwAJJk4D9gOk1tTPkoaAOcq31cK31cK316EitHZugBZC0OTAb+EFEPNqxhs3MCtfRsDczs+7wJKmZWQEc9mY2bEnaQtJsSVt1u5aB9EKdIyrsJc2TdLukk7pdSzNJoyX9WtIt+WcXSf8o6S5JF3S7vn6SJkpakG+PkXRNPqZHt1rWI7W+SNLDDce3Ly/v6nNC0qaSrpc0X9KVksZW1dTtOgeodbXnbN6uJ563krYGrgVeC3xfUl8vHtsWdXb8uI6YsB8Gl2PYFbg0ImZFxCxgHOlU1NcCD0vau5vFwfMT6F8hfQAO4P3AwnxM3yhpQotlvVDrbsDp/cc3Ipb2yHPi7cA5ETEbeBQ4ormmHqmzqtYTaHjORsR9kqbRO8/bVwB/HxGnAzcCe9Gbx7a5zqPpwnEdMWEPzAIuy7dvZtU5/b1iOnCopNskfZ30xPx2pBnym4A9ulpd8hzwVtInnGH1Y3o7MK3Fsm5ornU68F5Jd0j6TF42iy4/JyLiwoiYn+/2Ae+oqGlWxbKOq6j1WRqes5JGA6+nR563EXFTRNwp6fWkkNyHHjy2FXU+RReO60gK++bLMUzsYi1V7gL+MiJmAk+QPk3cU/VGxPKIWNawqOqY9sRxrqj1emBGROwOvFTSrvRIrQCSdgc2Bx6qqKln6oTVap3P6s/Z/em9WkV60X8GED16bJvq/CldOK4jKex7/XIM90bEI/n2A/R+vVBdY6/WfXtE/CHffgCYQo/UKmkL4LOkt+89fUybam1+zvbMMe0XyXGkd5nT6dFj21TnC7txXHvlP2o79PrlGC6RNFXSKOBQ0it5L9cL1ce0V4/zjZK2lrQh6e38f9IDtUoaSxpG+FhEPNiipq7XCZW1Nj9nf9ortQJI+qiko/LdzYAz6MFjW1HnF7pyXCNiRPwAm+SDdg7wM2DTbtfUVN/OwL3AfcDppBfaHwLnkS7nvH23a2yo9Zb8ezvg/lzjXcCoqmU9UuuepF7SvcD7euU5Afwd8DhwS/55V3NNvVBni1o/0ficzdv0zPOWVUNNPwAuzMey545tRZ27dOO4jqhP0GqYXY5B0njgAOCeiPhlt+uponQto5nAjZHHyKuW9apefE5U1dSLdbbSy8/b4Xxs6z6uIyrszcys2kgaszczsxYc9mZmBXDYW3HyWRDdaHdsN9o1A4e9FSZP1s2XNOBzX9L7JR07wPoDJJ3WcP98SQcNsP1OwDUN9zv5xUFmHf2mKrOu6A/WiHg2Ih6X9H3g1cDCvL6/p6+IeDbffoZ0uYDn99GwDtLlGp7NLxrnA09ExNVN7c4DdgCezIuelnQtqZP1J+CQ9v2VZgNz2FsJjgLeKenlpI+kPwXsk3vbDwFPA58Hjpb0DPAC4HUAkv6aFPpjJL214tS9TwE/iYgvV7T7HHBcbu+TEfGufIGrWcA/tfdPNBuYT720Ykg6kRTM1+X79wK7RcRTTdsdBxxJ6oGfHxGXNqx7K/ABYALpAzu/Jn3Y7Jm8yTjg5Ii4SdKXSS8mM4GdgHuArfLPLyLiwLr+VrNmDnsrhqQdSUH8rtzL/1REHNK0zZbAFcDledHhwEER8UTTdvuSrsVyKul6J7Mi4o9N23wN+DDpncI84PvAJOCXEXF2e/86s4F5gtaKERGLSBcgnA58hnQtledJGgd8DTiZNHSzAjgbuFItvmEoIlaSgvyMitWbAJOBuaQx+mmksH+NJ2it0xz2VpoPkS72tTQi7uxfKGkH4HukSdvTSV/S8g/AR4BbgbskvappX5vmoZrHgI0knZcvxNZvQm7jQNL82AdJF7y6ommy16x2DnsrhqSppAtRXUCacP2cpD/Pq1cC50XEyRGxR97u3IiYGRGfBPYlXVCr30uAY0jhfRXpImIrgIWSNsn7XQIQEX8C/oV04bhXk76izqyjPGZvI56kVwJfJF1l8MyI+EVevjdpsvVlwD4R8auGx3wAeCYiPt9in9sCG0fEfzUt3yginpR0ALCM9FV/m5EuWXslaShnT9JwzikR8b32/aVmrTnsbcTL3xK0UUSsaLF+bEQ83eGyzDrKYW9mVgCP2ZuZFcBhb2ZWAIe9mVkBHPZmZgX4fxS3LvM9hf2cAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 设置全局字体为SimHei，这是一种支持中文的字体  \n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "# 解决保存图像时负号'-'显示为方块的问题  \n",
    "plt.rcParams['axes.unicode_minus'] = False \n",
    "\n",
    "# 计算字符串长度并统计分布  \n",
    "length_counts = data_frame['text'].apply(len).value_counts().sort_index()  \n",
    "\n",
    "# 绘制直方图  \n",
    "plt.hist(length_counts.index, bins=len(length_counts), weights=length_counts.values)  \n",
    "plt.xlabel('文本长度')  \n",
    "plt.ylabel('频数')  \n",
    "plt.title('字符串长度分布直方图')  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载预训练模型（代码/手动下载）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载Tokenizer和Encoding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset_train): 612\n",
      "len(dataset_val): 108\n"
     ]
    }
   ],
   "source": [
    "# 下载的预训练文件路径\n",
    "BERT_PATH = '../Models/bert-base-chinese'\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n",
    "\n",
    "\n",
    "# 将strings转化为tokens\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    sub_df[sub_df.data_type=='train'].text.values,\n",
    "    add_special_tokens=True,    # This is just the BERT way of knowing that when the sentence ends and when the a new one begins.\n",
    "    return_attention_mask=True, # 为将不同长度的句子拥有相同的维度，将max_length设置为很大的数字256，\n",
    "    pad_to_max_length=True,      # attention_mask表示实际值在哪里，同时哪里为空值\n",
    "    max_length=256,\n",
    "    return_tensors='pt' # pt表示PyTorch\n",
    "    )\n",
    "encoded_data_val= tokenizer.batch_encode_plus(\n",
    "    sub_df[sub_df.data_type=='val'].text.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=256,\n",
    "    return_tensors='pt'\n",
    "    )\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(sub_df[sub_df.data_type=='train'].label.values)\n",
    "input_ids_val = encoded_data_val['input_ids']   # input_ids用数字表示每个word\n",
    "attention_masks_val= encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(sub_df[sub_df.data_type=='val'].label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "print(\"len(dataset_train):\", len(dataset_train))\n",
    "print(\"len(dataset_val):\", len(dataset_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../Models/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    BERT_PATH, \n",
    "    num_labels = len(label_dict), \n",
    "    output_attentions=False,    # 是否输出注意力分散\n",
    "    output_hidden_states=False  # 是否输出模型的隐藏状态\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16\n",
    "dataloader_train = DataLoader(dataset_train,\n",
    "                              sampler=RandomSampler(dataset_train),\n",
    "                              batch_size=batch_size)\n",
    "dataloader_val = DataLoader(dataset_val,\n",
    "                            sampler=RandomSampler(dataset_val),\n",
    "                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DHW\\.conda\\envs\\d2l\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                lr=1e-5, #2e-5 > 5e-5 \n",
    "                eps=1e-8)\n",
    "\n",
    "epochs=5\n",
    "# 线性学习率调度器，在训练的后续阶段线性下降\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    labels_dict_inverse = {v: k for k,v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds,axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {labels_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 8\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(device)\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 本地保存的模型参数路径\n",
    "save_path = \"./bert_checkpoint\"\n",
    "epoch_trained = 0  # 已保存的模型的epoch\n",
    "\n",
    "# 保存模型权重\n",
    "def save_model(epoch):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    save_name=f\"bert-base-chinese_epoch_{epoch}.model\"\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, save_name))\n",
    "\n",
    "# 从本地加载已经训练过的模型参数\n",
    "# model.load_state_dict(torch.load(f'{save_model}/bert-base-chinese_epoch_{epoch_trained}.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4292f72e7bc4be6895d83dc1525d786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f71f379db34b8c8b17b896f83a7a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 1.1330736478169758\n",
      "Validation loss: 1.0584240896361214\n",
      "F1 Score (weighted): 0.3174341849205237\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19dbd13965ef4946a29118c6b4a92088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 1.0097618362842462\n",
      "Validation loss: 0.9864071352141244\n",
      "F1 Score (weighted): 0.5675936067502333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8efd85a12b94112aa1dcbcb156036d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tqdm(range(epoch_trained+1, epoch_trained+1+epochs)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "    \n",
    "    progress_bar =  tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch),leave=False,disable=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {\n",
    "            'input_ids' : batch[0],\n",
    "            'attention_mask' : batch[1],\n",
    "            'labels' : batch[2]\n",
    "            \n",
    "        }\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss' : '{:.3f}'.format(loss.item()/len(batch))})\n",
    "\n",
    "    save_model(epoch=epoch) \n",
    "    # torch.save(model.state_dict(), f'Models/BERT_ft_epoch_{epoch}.model')\n",
    "    \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_val)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions, true_vals = evaluate(dataloader_val)\n",
    "accuracy_per_class(predictions, true_vals )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
